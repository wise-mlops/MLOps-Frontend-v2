const config = useAppConfig()

// ê¸°ë³¸ CRUD í•¨ìˆ˜ë“¤
export const getEndpoints = async (namespace: string | null) => {
  const url = encodeURI(`/inference-services`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url,
  })
  return response
}

export const getEndpointDetails = async (namespace: string | null, name: string | string[]) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url,
  })
  return response
}

export const removeEndpoint = async (namespace: string | null, name: string | string[]) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}`)
  const response = await $fetch<ResponseBody>(url, {
    method: "DELETE",
    baseURL: config.api.url,
    headers: { "Content-Type": "application/json" }
  })
  return response
}

// ìƒˆë¡œìš´ ë°°í¬ API í•¨ìˆ˜ë“¤
export const deployInferenceService = async (
  namespace: string,
  name: string,
  inferenceServiceInfo: any,
  serving_type: string = "standard",
  deployment_strategy: string = "blue-green",
  canary_traffic_percent?: number,
  additional_test_duration: number = 60
) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}/deploy`)
  const params = new URLSearchParams({
    serving_type,
    deployment_strategy,
    additional_test_duration: additional_test_duration.toString()
  })

  // canary ì „ëµì¼ ë•Œë§Œ canary_traffic_percent ì¶”ê°€
  if (deployment_strategy === 'canary' && canary_traffic_percent !== undefined) {
    params.append('canary_traffic_percent', canary_traffic_percent.toString())
  }

  const response = await $fetch<ResponseBody>(`${url}?${params.toString()}`, {
    method: 'POST',
    baseURL: config.api.url,
    headers: { "Content-Type": "application/json" },
    body: inferenceServiceInfo
  })
  return response
}

// ì „ëµë³„ Config ìƒì„± í•¨ìˆ˜
const createDeploymentConfig = (
  servingType: string,
  strategy: string,
  formData: any
) => {
  switch (strategy) {
    case 'blue-green':
      if (servingType === 'vllm') {
        // vLLM Blue-Green ìŠ¤í‚¤ë§ˆ (FRONTEND_INTEGRATION.md ê¸°ì¤€)
        const adapters = (formData.adapters || []).map((adapter: any, index: number) => ({
          adapterName: adapter.name || `adapter-${index}`,
          adapterPath: adapter.storage_uri || adapter.path,
          defaultAdapter: index === 0 // ì²« ë²ˆì§¸ ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ
        }))

        return {
          baseModelPath: formData.base_model?.storage_uri || formData.storage_uri,
          customContainer: formData.vllm_image_tag ? `vllm/vllm-openai:${formData.vllm_image_tag}` : 'vllm/vllm-openai:latest',
          adapters,
          maxModelLen: formData.vllm_max_model_len || formData.max_model_len,
          tensorParallelSize: formData.vllm_tensor_parallel_size || formData.tensor_parallel_size || 1,
          gpuMemoryUtilization: formData.vllm_gpu_memory_utilization || formData.gpu_memory_utilization || 0.9
        }
      } else {
        return {
          storageUri: formData.storage_uri,
          modelFormat: formData.model_format || 'sklearn'
        }
      }
    case 'canary':
      return {
        storageUri: formData.storage_uri,
        modelFormat: formData.model_format || 'sklearn',
        trafficPercent: formData.canary_traffic_percent || 20
      }
    case 'lora-adapter':
      return {
        loraName: formData.adapter_name,
        loraPath: formData.adapter_path
      }
    case 'modelmesh':
      return {
        modelPath: formData.model_path || formData.storage_uri,
        modelFormat: formData.model_format
      }
    default:
      throw new Error(`Unknown deployment strategy: ${strategy}`)
  }
}

export const redeployInferenceService = async (
  namespace: string,
  name: string,
  formData: any,
  servingType: string,
  strategy: string
) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}/deploy`)

  // DeploymentRequest êµ¬ì¡°ë¡œ ë³€í™˜
  const deploymentRequest = {
    servingType,
    strategy,
    config: createDeploymentConfig(servingType, strategy, formData),
    testPayload: formData.test_payload || null
  }

  console.log('ğŸŒ API ìš”ì²­ URL:', `${config.api.url}${url}`)
  console.log('ğŸ“¤ DeploymentRequest:', JSON.stringify(deploymentRequest, null, 2))

  try {
    console.log('â³ $fetch í˜¸ì¶œ ì‹œì‘...')
    const response = await $fetch<ResponseBody>(url, {
      method: 'POST',
      baseURL: config.api.url,
      headers: { "Content-Type": "application/json" },
      body: deploymentRequest,
      timeout: 60000 // 60ì´ˆ íƒ€ì„ì•„ì›ƒ
    })
    console.log('âœ… API ìš”ì²­ ì„±ê³µ:', response)
    return response
  } catch (error) {
    console.error('âŒ API ìš”ì²­ ì‹¤íŒ¨ - ìƒì„¸ ì •ë³´:', {
      error,
      type: typeof error,
      message: error instanceof Error ? error.message : 'Unknown error',
      stack: error instanceof Error ? error.stack : undefined
    })
    throw error
  }
}

// ë°°í¬ ë³´ê³ ì„œ API í•¨ìˆ˜ë“¤
export const getDeploymentReport = async (
  namespace: string,
  name: string,
  deployment_id: string
) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}/deployment-report/${deployment_id}`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url
  })
  return response
}

export const getDeploymentReports = async (namespace: string) => {
  const url = encodeURI(`/inference-services/${namespace}/deployment-reports`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url
  })
  return response
}

// ë°°í¬ ìƒíƒœ ì¡°íšŒ
export const getDeploymentStatus = async (
  namespace: string,
  name: string
) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}/deployment-status`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url
  })
  return response
}

// Inference Service ìƒì„± (basic.py API ì‚¬ìš©)
export const createEndpoint = async (namespace: string, name: string, inferenceServiceInfo: any) => {
  const url = encodeURI(`/inference-services/${namespace}/${name}`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'POST',
    baseURL: config.api.url,
    headers: { "Content-Type": "application/json" },
    body: inferenceServiceInfo
  })
  return response
}

// Kubernetes ê´€ë¦¬ í•¨ìˆ˜ë“¤ (í•„ìš”ì‹œ ìœ ì§€)
export const getEndpointPods = async (namespace: string, name: string) => {
  const url = encodeURI(`/k8s-managements/namespaces/${namespace}/inference-services/${name}/pods`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url,
  })
  return response
}

export const getEndpointEvents = async (namespace: string, name: string) => {
  const url = encodeURI(`/k8s-managements/namespaces/${namespace}/inference-services/${name}/events`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url,
  })
  return response
}

export const getInferenceServiceStatus = async (namespace: string, name: string) => {
  const url = encodeURI(`/k8s-managements/namespaces/${namespace}/inference-services/${name}/status`)
  const response = await $fetch<ResponseBody>(url, {
    method: 'GET',
    baseURL: config.api.url,
  })
  return response
}

// ì„œë¹™ íƒ€ì… ê°ì§€ í•¨ìˆ˜ (ê°„ì†Œí™”)
export const detectServingType = (endpointDetails: any): 'standard' | 'vllm' | 'modelmesh' => {
  const namespace = endpointDetails.result?.metadata?.namespace
  if (namespace === 'modelmesh-serving') {
    return 'modelmesh'
  }

  const predictor = endpointDetails.result?.spec?.predictor
  if (predictor?.containers?.[0]?.image?.includes('vllm')) {
    return 'vllm'
  }

  return 'standard'
}

// MLmodelì—ì„œ dtypeì„ MLFlow í˜•ì‹ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜
export const mapDtypeToMLFlow = (dtype: string): string => {
  const dtypeMap: Record<string, string> = {
    'int64': 'INT64',
    'int32': 'INT32',
    'float64': 'FP64',
    'float32': 'FP32',
    'double': 'FP64',
    'float': 'FP32',
    'long': 'INT64',
    'int': 'INT32'
  }

  return dtypeMap[dtype.toLowerCase()] || 'FP64'
}

// MLmodel íŒŒì¼ì—ì„œ signatureì˜ input dtypeì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜
export const getMLModelSignature = async (storageUri: string) => {
  try {
    let cleanUri = storageUri.replace(/^(s3:\/\/|minio:\/\/|gs:\/\/|\/\/)/, '')
    const firstSlashIndex = cleanUri.indexOf('/')
    if (firstSlashIndex === -1) {
      throw new Error('Invalid storage URI format')
    }
    
    const bucketName = cleanUri.substring(0, firstSlashIndex)
    const objectPath = cleanUri.substring(firstSlashIndex + 1)
    
    const mlModelPath = objectPath.endsWith('/') 
      ? `${objectPath}MLmodel`
      : `${objectPath}/MLmodel`
    
    const blob = await downloadObjects(bucketName, [mlModelPath])
    const text = await blob.text()
    
    // ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ signature inputs ë¶€ë¶„ ì¶”ì¶œ ì‹œë„
    let inputsStr = null
    
    // íŒ¨í„´ 1: ì‘ì€ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ê²½ìš°
    let match = text.match(/inputs:\s*'([^']+)'/s)
    if (match) {
      inputsStr = match[1]
    } else {
      // íŒ¨í„´ 2: í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ê²½ìš°  
      match = text.match(/inputs:\s*"([^"]+)"/s)
      if (match) {
        inputsStr = match[1].replace(/\\"/g, '"')
      } else {
        // íŒ¨í„´ 3: ë”°ì˜´í‘œ ì—†ì´ JSON í˜•íƒœ
        match = text.match(/inputs:\s*(\[.*?\])/s)
        if (match) {
          inputsStr = match[1]
        }
      }
    }
    
    if (!inputsStr) {
      throw new Error('No inputs found in signature')
    }
    
    const inputs = JSON.parse(inputsStr)
    
    // ì²« ë²ˆì§¸ inputì˜ dtypeê³¼ shape ì¶”ì¶œ
    if (inputs && inputs.length > 0) {
      const firstInput = inputs[0]
      const tensorSpec = firstInput['tensor-spec']
      
      if (tensorSpec && tensorSpec.dtype) {
        return {
          dtype: mapDtypeToMLFlow(tensorSpec.dtype),
          shape: tensorSpec.shape || [-1]
        }
      }
    }
    
    return null
    
  } catch (error) {
    console.error('Failed to fetch MLmodel:', error)
    return null
  }
}

export const getInputExample = async (storageUri: string) => {
  try {
    let cleanUri = storageUri.replace(/^(s3:\/\/|minio:\/\/|gs:\/\/|\/\/)/, '')
    const firstSlashIndex = cleanUri.indexOf('/')
    if (firstSlashIndex === -1) {
      throw new Error('Invalid storage URI format')
    }
    
    const bucketName = cleanUri.substring(0, firstSlashIndex)
    const objectPath = cleanUri.substring(firstSlashIndex + 1)
    
    const inputExamplePath = objectPath.endsWith('/') 
      ? `${objectPath}input_example.json`
      : `${objectPath}/input_example.json`
    
    const blob = await downloadObjects(bucketName, [inputExamplePath])

    const text = await blob.text()
    const jsonData = JSON.parse(text)
    
    return jsonData
    
  } catch (error) {
    console.error('Failed to fetch input_example.json:', error)
    throw error
  }
}

export const getEndpointWithInputExample = async (namespace: string | null, name: string | string[]) => {
  try {

    const endpointDetails = await getEndpointDetails(namespace, name)
    const storageUri = endpointDetails?.result.spec?.predictor?.model?.storageUri

    let inputExample = null
    let mlModelSignature = null
    
    if (storageUri) {
      try {
        inputExample = await getInputExample(storageUri)
      } catch (error) {
        console.warn('Could not load input example:', error)
      }
      
      try {
        mlModelSignature = await getMLModelSignature(storageUri)
      } catch (error) {
        console.error('Failed to load MLmodel signature:', error)
        mlModelSignature = null
      }
    }
    return {
      endpoint: endpointDetails,
      inputExample,
      mlModelSignature
    }
  } catch (error) {
    console.error('Failed to fetch endpoint with input example:', error)
    throw error
  }
}

// LLM ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
export const isLLMModel = (endpointDetails: any): boolean => {
  // VLLM ëŸ°íƒ€ì„ì´ë‚˜ LLM ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
  const containers = endpointDetails?.result?.spec?.predictor?.containers || []
  const envs = endpointDetails?.result?.spec?.predictor?.model?.env || []
  
  // containers ë°°ì—´ì—ì„œ VLLM ê´€ë ¨ ì´ë¯¸ì§€ë‚˜ ì„¤ì •ì´ ìˆëŠ”ì§€ í™•ì¸
  const hasVllmContainer = containers.some((container: any) => {
    const image = container?.image || ''
    const containerEnvs = container?.env || []
    
    // ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ì— vllmì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if (image.toLowerCase().includes('vllm')) {
      return true
    }
    
    // ì»¨í…Œì´ë„ˆ í™˜ê²½ë³€ìˆ˜ì— VLLM ê´€ë ¨ ì„¤ì •ì´ ìˆëŠ”ì§€ í™•ì¸
    return containerEnvs.some((env: any) => 
      env.name && env.name.toLowerCase().includes('vllm')
    )
  })
  
  if (hasVllmContainer) {
    return true
  }
  
  // ëª¨ë¸ ë ˆë²¨ í™˜ê²½ë³€ìˆ˜ì— VLLM ê´€ë ¨ ì„¤ì •ì´ ìˆìœ¼ë©´ LLMìœ¼ë¡œ íŒë‹¨
  const hasVllmEnv = envs.some((env: any) => 
    env.name && env.name.toLowerCase().includes('vllm')
  )
  
  if (hasVllmEnv) {
    return true
  }
  
  return false
}

// VLLM APIë¥¼ í†µí•œ LLM ì¶”ë¡ 
export const runLLMInference = async (
  namespace: string, 
  name: string, 
  prompt: string, 
  max_tokens: number = 256, 
  temperature: number = 0.7
) => {
  const url = `/inference-services/${namespace}/${name}/infer-vllm`
  
  const response = await $fetch<ResponseBody>(url, {
    method: 'POST',
    baseURL: config.api.url,
    body: {
      prompt: prompt,
      max_tokens: max_tokens,
      temperature: temperature
    },
    headers: {
      "Content-Type": "application/json",
      "wisenut-authorization": "miracle-wisenut",
    },
  })
  
  return response
}

// MLFlow ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
export const isMLFlowModel = (endpointDetails: any): boolean => {
  const predictor = endpointDetails?.result?.spec?.predictor
  const modelFormat = predictor?.model?.modelFormat?.name
  
  return modelFormat === 'mlflow'
}

// === Inference Result Processing Utilities ===

export const formatNumber = (value: any): string => {
  if (value === null || value === undefined) return "N/A";
  if (typeof value !== "number" || isNaN(value)) return String(value);
  try {
    return value.toLocaleString("en-US", {
      minimumFractionDigits: 2,
      maximumFractionDigits: 2,
    });
  } catch (error) {
    return String(value);
  }
}

export const hasLLMText = (result: any): boolean => {
  return !!(result?.choices?.[0]?.text || result?.result?.choices?.[0]?.text);
}

export const getLLMText = (result: any): string => {
  return result?.choices?.[0]?.text || result?.result?.choices?.[0]?.text || '';
}

export const getLLMUsage = (result: any) => {
  return result?.usage || result?.result?.usage;
}

// MLFlow ìˆ«ì ë°°ì—´ ê²°ê³¼ì¸ì§€ í™•ì¸ (result.resultê°€ ìˆ«ì ë°°ì—´ì¸ ê²½ìš°)
export const isNumericArrayResult = (result: any): boolean => {
  return !!(result?.result && Array.isArray(result.result) && result.result.length > 0 && typeof result.result[0] === 'number');
}

export const getResultTitle = (isMLFlow: boolean, result: any): string => {
  return !isMLFlow && hasLLMText(result) ? 'Generated Text' : 'Prediction';
}

export const isMLFlowArrayResult = (result: any): boolean => {
  return !!(result?.result && Array.isArray(result.result));
}

export const isDirectArrayResult = (result: any): boolean => {
  return !!(result && Array.isArray(result));
}

export const isObjectResult = (result: any): boolean => {
  return !!(result && typeof result === 'object' && !Array.isArray(result));
}


